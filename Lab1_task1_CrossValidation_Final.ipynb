{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "### Run two classification algorithms--Naïve Bayes (NB) and Random Forest (RF)--on the Primate splice-junction gene sequences.\n",
    "\n",
    "### Primate splice-junction gene sequences dataset contains no missing values ans data is nominal ,that  is why using Multinomial type of Naive Bayes classifier. \n",
    "\n",
    "### Data has instance name,values and category,total 62 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3190 data points:\n",
      "[ ('ATRINS-DONOR-521', 'C', 'C', 'A', 'G', 'C', 'T', 'G', 'C', 'A', 'T', 'C', 'A', 'C', 'A', 'G', 'G', 'A', 'G', 'G', 'C', 'C', 'A', 'G', 'C', 'G', 'A', 'G', 'C', 'A', 'G', 'G', 'T', 'C', 'T', 'G', 'T', 'T', 'C', 'C', 'A', 'A', 'G', 'G', 'G', 'C', 'C', 'T', 'T', 'C', 'G', 'A', 'G', 'C', 'C', 'A', 'G', 'T', 'C', 'T', 'G', 'EI')\n",
      " ('ATRINS-DONOR-905', 'A', 'G', 'A', 'C', 'C', 'C', 'G', 'C', 'C', 'G', 'G', 'G', 'A', 'G', 'G', 'C', 'G', 'G', 'A', 'G', 'G', 'A', 'C', 'C', 'T', 'G', 'C', 'A', 'G', 'G', 'G', 'T', 'G', 'A', 'G', 'C', 'C', 'C', 'C', 'A', 'C', 'C', 'G', 'C', 'C', 'C', 'C', 'T', 'C', 'C', 'G', 'T', 'G', 'C', 'C', 'C', 'C', 'C', 'G', 'C', 'EI')\n",
      " ('BABAPOE-DONOR-30', 'G', 'A', 'G', 'G', 'T', 'G', 'A', 'A', 'G', 'G', 'A', 'C', 'G', 'T', 'C', 'C', 'T', 'T', 'C', 'C', 'C', 'C', 'A', 'G', 'G', 'A', 'G', 'C', 'C', 'G', 'G', 'T', 'G', 'A', 'G', 'A', 'A', 'G', 'C', 'G', 'C', 'A', 'G', 'T', 'C', 'G', 'G', 'G', 'G', 'G', 'C', 'A', 'C', 'G', 'G', 'G', 'G', 'A', 'T', 'G', 'EI')\n",
      " ...,\n",
      " ('ORARGIT-NEG-241', 'T', 'C', 'T', 'C', 'G', 'G', 'G', 'G', 'G', 'C', 'G', 'G', 'C', 'C', 'G', 'G', 'C', 'G', 'C', 'G', 'G', 'C', 'G', 'G', 'G', 'G', 'A', 'G', 'C', 'G', 'G', 'T', 'C', 'C', 'C', 'C', 'G', 'G', 'C', 'C', 'G', 'C', 'G', 'G', 'C', 'C', 'C', 'C', 'G', 'A', 'C', 'G', 'T', 'G', 'T', 'G', 'T', 'G', 'T', 'C', 'N')\n",
      " ('TARHBB-NEG-541', 'A', 'T', 'T', 'C', 'T', 'A', 'C', 'T', 'T', 'A', 'G', 'T', 'A', 'A', 'A', 'C', 'A', 'T', 'A', 'A', 'T', 'T', 'T', 'C', 'T', 'T', 'G', 'T', 'G', 'C', 'T', 'A', 'G', 'A', 'T', 'A', 'A', 'C', 'C', 'A', 'A', 'A', 'T', 'T', 'A', 'A', 'G', 'A', 'A', 'A', 'A', 'C', 'C', 'A', 'A', 'A', 'A', 'C', 'A', 'A', 'N')\n",
      " ('TARHBD-NEG-1981', 'A', 'G', 'G', 'C', 'T', 'G', 'C', 'C', 'T', 'A', 'T', 'C', 'A', 'G', 'A', 'A', 'G', 'G', 'T', 'G', 'G', 'T', 'G', 'G', 'C', 'T', 'G', 'G', 'T', 'G', 'T', 'G', 'G', 'C', 'T', 'G', 'C', 'T', 'G', 'C', 'T', 'C', 'T', 'G', 'G', 'C', 'T', 'C', 'A', 'C', 'A', 'A', 'G', 'T', 'A', 'C', 'C', 'A', 'T', 'T', 'N')]\n"
     ]
    }
   ],
   "source": [
    "#load dataset and printing data\n",
    "#importig module which can load arff file\n",
    "\n",
    "from scipy.io.arff import loadarff\n",
    "with open(\"C:\\Users\\Pankaj\\Desktop\\Lab1\\Lab1\\data\\splice.arff\", \"r\") as f:\n",
    "    data,m = loadarff(f)\n",
    "print(\"There are %d data points:\" % (data.size))\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ('C', 'C', 'A', 'G', 'C', 'T', 'G', 'C', 'A', 'T', 'C', 'A', 'C', 'A', 'G', 'G', 'A', 'G', 'G', 'C', 'C', 'A', 'G', 'C', 'G', 'A', 'G', 'C', 'A', 'G', 'G', 'T', 'C', 'T', 'G', 'T', 'T', 'C', 'C', 'A', 'A', 'G', 'G', 'G', 'C', 'C', 'T', 'T', 'C', 'G', 'A', 'G', 'C', 'C', 'A', 'G', 'T', 'C', 'T', 'G')\n",
      " ('A', 'G', 'A', 'C', 'C', 'C', 'G', 'C', 'C', 'G', 'G', 'G', 'A', 'G', 'G', 'C', 'G', 'G', 'A', 'G', 'G', 'A', 'C', 'C', 'T', 'G', 'C', 'A', 'G', 'G', 'G', 'T', 'G', 'A', 'G', 'C', 'C', 'C', 'C', 'A', 'C', 'C', 'G', 'C', 'C', 'C', 'C', 'T', 'C', 'C', 'G', 'T', 'G', 'C', 'C', 'C', 'C', 'C', 'G', 'C')\n",
      " ('G', 'A', 'G', 'G', 'T', 'G', 'A', 'A', 'G', 'G', 'A', 'C', 'G', 'T', 'C', 'C', 'T', 'T', 'C', 'C', 'C', 'C', 'A', 'G', 'G', 'A', 'G', 'C', 'C', 'G', 'G', 'T', 'G', 'A', 'G', 'A', 'A', 'G', 'C', 'G', 'C', 'A', 'G', 'T', 'C', 'G', 'G', 'G', 'G', 'G', 'C', 'A', 'C', 'G', 'G', 'G', 'G', 'A', 'T', 'G')\n",
      " ...,\n",
      " ('T', 'C', 'T', 'C', 'G', 'G', 'G', 'G', 'G', 'C', 'G', 'G', 'C', 'C', 'G', 'G', 'C', 'G', 'C', 'G', 'G', 'C', 'G', 'G', 'G', 'G', 'A', 'G', 'C', 'G', 'G', 'T', 'C', 'C', 'C', 'C', 'G', 'G', 'C', 'C', 'G', 'C', 'G', 'G', 'C', 'C', 'C', 'C', 'G', 'A', 'C', 'G', 'T', 'G', 'T', 'G', 'T', 'G', 'T', 'C')\n",
      " ('A', 'T', 'T', 'C', 'T', 'A', 'C', 'T', 'T', 'A', 'G', 'T', 'A', 'A', 'A', 'C', 'A', 'T', 'A', 'A', 'T', 'T', 'T', 'C', 'T', 'T', 'G', 'T', 'G', 'C', 'T', 'A', 'G', 'A', 'T', 'A', 'A', 'C', 'C', 'A', 'A', 'A', 'T', 'T', 'A', 'A', 'G', 'A', 'A', 'A', 'A', 'C', 'C', 'A', 'A', 'A', 'A', 'C', 'A', 'A')\n",
      " ('A', 'G', 'G', 'C', 'T', 'G', 'C', 'C', 'T', 'A', 'T', 'C', 'A', 'G', 'A', 'A', 'G', 'G', 'T', 'G', 'G', 'T', 'G', 'G', 'C', 'T', 'G', 'G', 'T', 'G', 'T', 'G', 'G', 'C', 'T', 'G', 'C', 'T', 'G', 'C', 'T', 'C', 'T', 'G', 'G', 'C', 'T', 'C', 'A', 'C', 'A', 'A', 'G', 'T', 'A', 'C', 'C', 'A', 'T', 'T')]\n"
     ]
    }
   ],
   "source": [
    "#first element is instance name and last one is class\n",
    "#slicing data without instance name and class\n",
    "X = data[m.names()[1:-1]]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "### changing character data into interger values because Naive bayes classifier runs on int Datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|S1\n",
      "[['C' 'C' 'A' ..., 'C' 'T' 'G']\n",
      " ['A' 'G' 'A' ..., 'C' 'G' 'C']\n",
      " ['G' 'A' 'G' ..., 'A' 'T' 'G']\n",
      " ..., \n",
      " ['T' 'C' 'T' ..., 'G' 'T' 'C']\n",
      " ['A' 'T' 'T' ..., 'C' 'A' 'A']\n",
      " ['A' 'G' 'G' ..., 'A' 'T' 'T']]\n",
      "[[67 67 65 ..., 67 84 71]\n",
      " [65 71 65 ..., 67 71 67]\n",
      " [71 65 71 ..., 65 84 71]\n",
      " ..., \n",
      " [84 67 84 ..., 71 84 67]\n",
      " [65 84 84 ..., 67 65 65]\n",
      " [65 71 71 ..., 65 84 84]]\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X1= [] #empty list\n",
    "\n",
    "import numpy as np\n",
    "import numpy as array\n",
    "\n",
    "X2 = np.asarray(X.tolist())\n",
    "\n",
    "print(X2.dtype)\n",
    "print(X2)\n",
    "\n",
    "for k in X2:\n",
    "   \n",
    "   x = np.array(list(map(ord,k)))#converting character type data into sequence by ordinal integer values.\n",
    "   X1.append(x)\n",
    "X1 = np.asarray(X1)    \n",
    "   \n",
    "print(X1)\n",
    "print(X1.dtype)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EI' 'EI' 'EI' ..., 'N' 'N' 'N']\n"
     ]
    }
   ],
   "source": [
    "#target class\n",
    "\n",
    "Y = data[m.names()[-1]]\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import  module to run naive bayes classifier\n",
    "#Data values are nominal(labeled) so using Multinomial type of NB classifier.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#creating object\n",
    "gnb = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Byes Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.75273865  0.76212833  0.76489028  0.76923077  0.73940345]\n",
      "0.757678296941\n"
     ]
    }
   ],
   "source": [
    "#5 fold\n",
    " \n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(gnb, X1, Y,scoring='accuracy', cv=5)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.759375    0.725       0.7375      0.790625    0.7625      0.78056426\n",
      "  0.80877743  0.71698113  0.75394322  0.75394322]\n",
      "0.75892092602\n"
     ]
    }
   ],
   "source": [
    "#10 fold\n",
    "\n",
    "scores = cross_val_score(gnb, X1, Y,scoring='accuracy', cv=10)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.7627907   0.74883721  0.70093458  0.73239437  0.79342723  0.77830189\n",
      "  0.71698113  0.85377358  0.75943396  0.80660377  0.77358491  0.70283019\n",
      "  0.77358491  0.73584906  0.75      ]\n",
      "0.759288498592\n"
     ]
    }
   ],
   "source": [
    "#15 fold\n",
    "\n",
    "scores = cross_val_score(gnb, X1, Y,scoring='accuracy', cv=15)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.89358372  0.87793427  0.89498433  0.91836735  0.90894819]\n",
      "0.898763572898\n"
     ]
    }
   ],
   "source": [
    "#5 fold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "frt = RandomForestClassifier()\n",
    "scores = cross_val_score(frt,X1,Y,scoring='accuracy',cv = 5)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.909375    0.921875    0.909375    0.86875     0.915625    0.89028213\n",
      "  0.9153605   0.90566038  0.89589905  0.92744479]\n",
      "0.905964685917\n"
     ]
    }
   ],
   "source": [
    "#10 fold\n",
    "\n",
    "frt = RandomForestClassifier()\n",
    "scores = cross_val_score(frt,X1,Y,scoring='accuracy',cv = 10)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90697674  0.89302326  0.92990654  0.87793427  0.92488263  0.88679245\n",
      "  0.93867925  0.91037736  0.89622642  0.95283019  0.9009434   0.91981132\n",
      "  0.93396226  0.85849057  0.93396226]\n",
      "0.910986594344\n"
     ]
    }
   ],
   "source": [
    "#15 fold\n",
    "\n",
    "frt = RandomForestClassifier()\n",
    "scores = cross_val_score(frt,X1,Y,scoring='accuracy',cv = 15)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Do the number of folds have any correlation with the number and percentage of correctly classified instances within the same model (For example, 5 folds and 10 folds in NB and RF respectively)? Explain the results.\n",
    "\n",
    "Answer:In same model with different fold number in naive bayes classifier getting similar accuracy aroung 75%\n",
    "\n",
    "cv = 5 means we are calling  machine learning algorithm 5 times , each time giving a different train/test set. \n",
    "\n",
    "we will then need to calculate the accuracy for each stage, and average it at the end which is the mean value.\n",
    "\n",
    " \n",
    "In RandomForest for each iteration getting different results.\n",
    "\n",
    "Number of folds impoves accuracy in each increase of fold size.Accuracy of 15 fold is somewhat higher than 5 fold.\n",
    "\n",
    "### 2.Do the same number of folds when applied to different models have any effect on the number and percentage of correctly classified instances (For example, 5 folds and 10 folds in NB and RF)? Explain the results.\n",
    "\n",
    "In the given data set random classifier out performs for all the 3 fold numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.Select 1 set of results generated for each classifier. For example, if you performed a test by selecting 10 folds, select the results you obtained for 10 folds for both–NB and RF. Considering all classes in the dataset; calculate the accuracy and error rate for the results of NB and RF. ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating accuracy and error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold:  [ 0.759375    0.725       0.7375      0.790625    0.7625      0.78056426\n",
      "  0.80877743  0.71698113  0.75394322  0.75394322]\n",
      "Average:  0.75892092602\n"
     ]
    }
   ],
   "source": [
    "#Naive bayes\n",
    "#10 fold\n",
    "\n",
    "scores = cross_val_score(gnb, X1, Y,scoring='accuracy', cv=10)\n",
    "print'10-fold: ',scores\n",
    "print 'Average: ',scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EI' 'EI' 'EI' ..., 'EI' 'IE' 'N']\n",
      "[[ 602   44  121]\n",
      " [  72  432  264]\n",
      " [ 138  130 1387]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = cross_val_predict(gnb,X1,Y,cv=10)\n",
    "print(Y_pred)\n",
    "conf_mat = confusion_matrix(Y,Y_pred)\n",
    "print (conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxgSLiSAogAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8913128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visulization of confusion matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "conf = confusion_matrix(Y, Y_pred)\n",
    "plt.imshow(conf, cmap='binary', interpolation='None')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination of confusion Matrix\n",
    "\n",
    "#### Black potion in the plot(between 1.5 tp 2.5) indicates class 'n' is correctly classified as 'n' 1387 times.\n",
    "####  white color is the indication of missclassified classes.for example white square in first row indicates 44 times class'ie' is classified as 'ei'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.758934169279\n"
     ]
    }
   ],
   "source": [
    "#accuracy\n",
    "from sklearn import metrics\n",
    "print metrics.accuracy_score(Y,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.241065830721\n"
     ]
    }
   ],
   "source": [
    "#error rate\n",
    "\n",
    "e = metrics.accuracy_score(Y,Y_pred)\n",
    "print 1-e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating accuracy and error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold:  [ 0.925       0.90625     0.88125     0.903125    0.946875    0.86206897\n",
      "  0.92163009  0.90566038  0.91798107  0.91798107]\n",
      "Average:  0.908782158203\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "#10 fold\n",
    "\n",
    "scores = cross_val_score(frt,X1,Y,scoring='accuracy',cv = 10)\n",
    "print'10-fold: ',scores\n",
    "print 'Average: ',scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EI' 'EI' 'EI' ..., 'N' 'N' 'N']\n",
      "['EI' 'EI' 'EI' ..., 'N' 'N' 'N']\n",
      "[[ 677   32   58]\n",
      " [  42  661   65]\n",
      " [  42   63 1550]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y_pred = cross_val_predict(frt,X1,Y,cv=10)\n",
    "print(Y_pred)\n",
    "print(Y)\n",
    "conf_mat = confusion_matrix(Y,Y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.905329153605\n"
     ]
    }
   ],
   "source": [
    "#accuracy\n",
    "\n",
    "print metrics.accuracy_score(Y,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.094670846395\n"
     ]
    }
   ],
   "source": [
    "#error rate\n",
    "\n",
    "e = metrics.accuracy_score(Y,Y_pred)\n",
    "print 1-e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the formula and explain the steps in calculating the accuracy and error-rate. Hint: compute the values of the confusion matrix first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2421\n",
      "3190\n",
      "0.758934169279\n"
     ]
    }
   ],
   "source": [
    "print 602+432+1387\n",
    "print 602   +44 +  121 +72  + 432  +  264+ 138 + 130 + 1387\n",
    "print 2421/float(3190)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula:Overall Accuarcy = sum of correct classification (TP:True Positives) / total number of classifications\n",
    "#### Naive Bayes Classifier\n",
    "[[ 602   44  121]\n",
    " [  72  432  264]\n",
    " [ 138  130 1387]]\n",
    "\n",
    "\n",
    "From confusion matrix diagonal value = sum of correct classification values = 602+432+1387 = 2421\n",
    "\n",
    "\n",
    "total = 3190\n",
    "Hence Accuracy = 2421/3190 = 0.758934169279\n",
    "\n",
    "Similarally,\n",
    "#### Random Forest Classifier\n",
    "\n",
    "Every time getting different accuracy but calculation method is same as above.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Percentage Split\n",
    "\n",
    "### Run the NB classifier by selecting 3 different percentages of training data (for example, run 3 different runs by selecting a testing-training split of 40%-60%, 55%-45%, 65%-35%, etc. in each run).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB classifier 40%-60%, 55%-45%, 65%-35%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40%-60%: [ 0.771518    0.72727273  0.77080063]\n",
      "0.756530450695\n",
      "55%-65%: [ 0.74530271  0.74947808  0.77148847]\n",
      "0.75542308764\n",
      "65%-35%: [ 0.72727273  0.76280323  0.74123989]\n",
      "0.743771951319\n"
     ]
    }
   ],
   "source": [
    "#To split data  testing-training split of 40%-60%\n",
    "#random_state parameter will make sure each time get consistent result.\n",
    "from sklearn.model_selection import train_test_split   \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X1, Y, test_size=0.40,random_state=42)\n",
    "\n",
    "\n",
    "scores = cross_val_score(gnb, X_train, Y_train)\n",
    "print'40%-60%:',scores\n",
    "print(scores.mean())\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X1, Y, test_size=0.55,random_state=42)\n",
    "\n",
    "\n",
    "scores = cross_val_score(gnb, X_train, Y_train)\n",
    "print'55%-65%:',scores\n",
    "print(scores.mean())\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X1, Y, test_size=0.65,random_state=42)\n",
    "\n",
    "scores = cross_val_score(gnb, X_train, Y_train)\n",
    "print'65%-35%:',scores\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the RF classifier by selecting the same set of training set ratios selected for the NB classifier.\n",
    "\n",
    "### Random Forest classifier 40%-60%, 55%-45%, 65%-35%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40%-60%: [ 0.89827856  0.89184953  0.89010989]\n",
      "0.893412660047\n",
      "55%-65%: [ 0.87891441  0.86221294  0.8490566 ]\n",
      "0.863394650806\n",
      "65%-35%: [ 0.88770053  0.86522911  0.87601078]\n",
      "0.876313475648\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X1, Y, test_size=0.40,random_state=42)\n",
    "\n",
    "\n",
    "scores = cross_val_score(frt, X_train, Y_train)\n",
    "print'40%-60%:',scores\n",
    "print(scores.mean())\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X1, Y, test_size=0.55,random_state=42)\n",
    "\n",
    "\n",
    "scores = cross_val_score(frt, X_train, Y_train)\n",
    "print'55%-65%:',scores\n",
    "print(scores.mean())\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X1, Y, test_size=0.65,random_state=42)\n",
    "\n",
    "scores = cross_val_score(frt, X_train, Y_train)\n",
    "print'65%-35%:',scores\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix based accuracy and error rate 40%-60%\n",
    "\n",
    "### Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit to dataset\n",
    "Y_pred = gnb.fit(X_train, Y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[386  24  95]\n",
      " [ 34 281 179]\n",
      " [ 91  86 898]]\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(Y_test,Y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.75458052073288329)\n",
      "('error rate:', 0.24541947926711671)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))  #accuracy\n",
    "print(\"error rate:\",1-metrics.accuracy_score(Y_test, Y_pred))  #error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit to dataset\n",
    "Y_pred = frt.fit(X_train, Y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[457  27  21]\n",
      " [ 22 443  29]\n",
      " [ 47  47 981]]\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(Y_test,Y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.90694310511089682)\n",
      "('error rate:', 0.093056894889103181)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))  #accuracy\n",
    "print(\"error rate:\",1-metrics.accuracy_score(Y_test, Y_pred))  #error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.Does the percentage of training data affect the classifier accuracy? How and why?\n",
    "\n",
    "for Naive bayes classifier accuracy slightly decreases with the increase of split of test size.\n",
    "\n",
    "\n",
    "Random forest behaves same way,with the increase of test size accuracy decreases.But getting different accuarcy values each time \n",
    "\n",
    "\n",
    "### 2.If the same percentage of testing data is used for both classifiers (e.g. 40% for both NB and RF, does the classification accuracy vary from one classifier to another? Why?\n",
    "\n",
    "Yes,Random forest classifier works better in multi class classification.\n",
    "\n",
    "### 3.Select any one set of results generated for each classifier. For example, if you performed a test by selecting 40% training data, select the results you obtained for 40% for both – NB and RF. Considering all classes in the dataset, calculate the accuracy and error rate for the results of NB and RF. Show the formula and explain the steps in calculating the accuracy and error-rate.\n",
    "\n",
    "Naive Bayes Classifier\n",
    "\n",
    "[[386  24  95]\n",
    " [ 34 281 179]\n",
    " [ 91  86 898]]\n",
    "\n",
    "\n",
    "From confusion matrix diagonal value = sum of correct classification values = 386 + 281 + 898 = 1565\n",
    "\n",
    "total = 3190 Hence Accuracy = 1565/2074 = 0.75458052073288329\n",
    "\n",
    "Similarally,\n",
    "\n",
    "Random Forest Classifier\n",
    "\n",
    "Every time getting different accuracy but calculation method is same as above.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
