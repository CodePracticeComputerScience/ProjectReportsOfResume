{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:\n",
    "\n",
    "Run the DBSCAN algorithm on the Image Segmentation data using 3 values of eps: 0.3, 0.6 and 1.0 for 3 different values of min_samples: 3, 5, and 7 (in total you would need to run it 9 times â€“ 3 for each) with a 40% percentage split.\n",
    "\n",
    "1.Is there any correlation between the eps values (keeping the min_samples constant) and the number of clustered and un-clustered instances (noise)?\n",
    "\n",
    "2.As we increase both, the eps and min_samples values, what trend do you observe in the number of un-clustered instances (noise)? Why do you think this occurs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2310 data points:\n",
      "[ 1.24913853e+02  1.23417316e+02  9.00000000e+00  1.43338000e-02\n",
      "  4.71380000e-03  1.89393964e+00  5.70932019e+00  2.42472330e+00\n",
      "  8.24369225e+00  3.70515950e+01  3.28213087e+01  4.41878774e+01\n",
      "  3.41456008e+01 -1.26908604e+01  2.14088501e+01 -8.71798933e+00\n",
      "  4.51374684e+01  4.26892977e-01 -1.36289695e+00]\n"
     ]
    }
   ],
   "source": [
    "#Load data segment.arff\n",
    "\n",
    "from scipy.io.arff import loadarff\n",
    "with open(\"segment.arff\", \"r\") as f:\n",
    "    data, meta = loadarff(f)\n",
    "#print(data)\n",
    "\n",
    "\n",
    "\n",
    "print(\"There are %d data points:\" % (data.size))       # Printing the data size\n",
    "X = data[meta.names()[:-1]]                            # matrix of datasize\n",
    "Y = data[meta.names()[-1]]\n",
    "\n",
    "\n",
    "import numpy as np                                  # data type of every element to float.\n",
    "X = np.asarray(X.tolist(), dtype=np.float_)\n",
    "print(X.mean(axis =0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.2761887   0.94973634  0.         ...  0.70107202 -0.47269334\n",
      "  -0.4386101 ]\n",
      " [-0.16333606  0.11453842  0.         ... -0.99229686  2.51076426\n",
      "  -0.49213757]\n",
      " [ 1.05683255 -1.43405774  0.         ...  2.20543198 -0.99687194\n",
      "  -0.60648701]\n",
      " ...\n",
      " [-0.61575812 -0.89465908  0.         ...  0.68294676 -0.4919258\n",
      "  -0.47075894]\n",
      " [-0.36898245  0.16673829  0.         ... -0.98711841  2.51076426\n",
      "  -0.49213757]\n",
      " [-1.45205346  0.41033768  0.         ... -0.88872696  1.25442494\n",
      "  -0.0729728 ]]\n",
      "[ 9.99681338e-18  1.16188925e-16  0.00000000e+00 -8.01042734e-16\n",
      " -1.51321956e-16  3.60510083e-16 -6.96893241e-19  7.81385538e-16\n",
      " -3.30868089e-16  9.80216389e-16 -7.38226219e-16  6.39844118e-16\n",
      " -3.40516456e-16  1.76972434e-15 -1.21538181e-15  7.62545390e-16\n",
      " -1.24383428e-16 -3.74976625e-15  7.56645828e-16]\n"
     ]
    }
   ],
   "source": [
    "#Standardization makes data in same unit for clustering purpose.\n",
    "\n",
    "\n",
    "#standardization\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "print(X_scaled)\n",
    "\n",
    "#mean value is nearly equal to 0 in all the data points after standardization\n",
    "print(X_scaled.mean(axis =0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924\n",
      "1386\n"
     ]
    }
   ],
   "source": [
    "# Compute DBSCAN\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "#import module for visulization\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#to display visulization on this page instead of background\n",
    "%matplotlib inline\n",
    "\n",
    "#to split whole dataset into traning and testing data\n",
    "\n",
    "from sklearn.model_selection import train_test_split   \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.60, random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "#cluster = 3,eps = 0.3\n",
    "db = DBSCAN(eps=0.3, min_samples=3).fit(X_train)\n",
    "\n",
    "#core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "#core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    " \n",
    " \n",
    "    \n",
    "#print(labels)\n",
    "\n",
    "#Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "#cluster = 3,eps = 0.6\n",
    "db = DBSCAN(eps=0.6, min_samples=3).fit(X_train)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    " \n",
    " \n",
    "    \n",
    "#print(labels)\n",
    "\n",
    "#Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "#cluster = 3,eps = 1.0\n",
    "db = DBSCAN(eps=1.0, min_samples=3).fit(X_train)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    " \n",
    " \n",
    "    \n",
    "#print(labels)\n",
    "\n",
    "#Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "#cluster =5 ,eps = 0.3\n",
    "db = DBSCAN(eps=0.3, min_samples=5).fit(X_train)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    " \n",
    " \n",
    "    \n",
    "#print(labels)\n",
    "\n",
    "#Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "#cluster = 5,eps = 0.6\n",
    "db = DBSCAN(eps=0.6, min_samples=5).fit(X_train)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    " \n",
    " \n",
    "    \n",
    "#print(labels)\n",
    "\n",
    "#Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "#cluster = 5,eps = 1.0\n",
    "db = DBSCAN(eps=1.0, min_samples=5).fit(X_train)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    " \n",
    " \n",
    "    \n",
    "#print(labels)\n",
    "\n",
    "#Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#cluster = 7,eps = 0.3\n",
    "db = DBSCAN(eps=0.3, min_samples=7).fit(X_train)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    " \n",
    " \n",
    "    \n",
    "#print(labels)\n",
    "\n",
    "#Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "#cluster = 7,eps = 0.6\n",
    "db = DBSCAN(eps=0.6, min_samples=7).fit(X_train)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    " \n",
    " \n",
    "    \n",
    "#print(labels)\n",
    "\n",
    "#Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#cluster = 7,eps = 1.0\n",
    "db = DBSCAN(eps=1.0, min_samples=7).fit(X_train)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    " \n",
    " \n",
    "    \n",
    "#print(labels)\n",
    "\n",
    "#Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion : \n",
    "Low min_samples forms more number of cluster but when eps value increases number of clusters decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
